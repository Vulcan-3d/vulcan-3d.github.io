<!DOCTYPE html>

<head>
    <meta charset="utf-8">
    <title>VULCAN-3D</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/slider.css">
    <link href="https://fonts.googleapis.com/css?family=Pacifico" rel="stylesheet">
    <!-- <meta name="viewport" content="width=device-width"> -->
</head>

<body>
    <div id="body">

        <h1 id="title"></h1>
        <br>
        <div id="author-list">
        </div>
        <br>
        <div id="affiliation-list">
        </div>
        <br>
        <div id="conference">
        </div>
        <br>
        <div id="button-list">
            <a id="paper">
                <span>Paper</span>
            </a>
            <a id="arxiv">
                <span>ArXiv</span>
            </a>
            <!-- <a id="code" href="https://github.com/Vulcan-3D/TBD">
                <span>Code</span>
            </a> -->
            <!-- <a id="code">
                <img src="assets/logos/arXiv.svg">
                <span>code</span>
            </a> -->
        </div>
        <div id="content" style="max-width:1200px;margin:auto; margin-bottom: 1em" >
            <br>
            <section class="slider-wrapper">
                <button class="slide-arrow slide-arrow-prev">
                            &#8249;
                </button>
                <button class="slide-arrow slide-arrow-next">
                            &#8250;
                </button>

                <ul class="slides-container">
                    <li class="slide">
                        <table style="margin-left:auto;margin-right:auto;">
                            <video autoplay muted loop height=500px>
                                <source src="assets/videos/demo_2.mp4">
                            </video>
                        </table>
                    </li>
                    <li class="slide">
                        <table style="margin-left:auto;margin-right:auto;">
                            <video autoplay muted loop height=500px>
                                <source src="assets/videos/demo_3.mp4">
                            </video>
                        </table>
                    </li>
                    <li class="slide">
                        <table style="margin-left:auto;margin-right:auto;">
                            <video autoplay muted loop height=500px>
                                <source src="assets/videos/demo_7.mp4">
                            </video>
                        </table>
                    </li>
                    <li class="slide">
                        <table style="margin-left:auto;margin-right:auto;">
                            <video autoplay muted loop height=500px>
                                <source src="assets/videos/demo_22.mp4">
                            </video>
                        </table>
                    </li>
                    <li class="slide">
                        <table style="margin-left:auto;margin-right:auto;">
                            <video autoplay muted loop height=500px>
                                <source src="assets/videos/demo_5.mp4">
                            </video>
                        </table>
                    </li>
                    <li class="slide">
                        <table style="margin-left:auto;margin-right:auto;">
                            <video autoplay muted loop height=500px>
                                <source src="assets/videos/demo_8.mp4">
                            </video>
                        </table>
                    </li>
                </ul>
            </section>

            <div id='method'> 
                <h2>Abstract</h2>
                <p style="max-width:1200px; font-size:18px; margin:auto; text-align: justify; margin-bottom: 1em;">Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM's 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines.</p>
            </div>
            <br>
            <div id='teaser'> 
                <h2>Interactive 3D Object Arrangement</h2>
                <p style="max-width:1200px; font-size:18px; margin:auto; text-align: justify; margin-bottom: 1em;">
                    Given an input scene and text prompt, our system iteratively transforms instructions 
                    into precise 3D actions. 
                    A team of <b><i>Collaborative Agents</i></b> leverages specialized <b><i>3D Scene Tools</i></b> 
                    to manipulate objects step-by-step, producing a final arrangement that satisfies the user's request.
                </p>
                <img src="assets/imgs/teaser_arxiv_crop.png" width=1200px>
            </div>
            <br>
            <div id='method'> 
                <h2>Method Overview</h2>
                <p style="max-width:1200px; font-size:18px; margin:auto; text-align: justify; margin-bottom: 1em;">
                    Our method solves a long-horizon task through an iterative multi-agent process. 
                    The <b><i>Planner</i></b> agent examines the global context (the user instruction and all previous rendered states) to formulate a concrete plan for the current movement. 
                    The <b><i>Executor</i></b> implements this single-step plan in the 3D scene using API tools and solvers.
                    A set of <b><i>Evaluators</i></b> and an automatic floating check assess the execution quality. 
                    The entire <b><i>Plan-Execute-Evaluate</i></b> loop repeats until the <b><i>Planner</i></b> validates that the final arrangement fulfills the original user instruction.
                </p>
                <img src="assets/imgs/overview.png" width=1200px>
            </div>
            <br>
            <div id="gallery">
                <h2>Results</h2>
                <!-- <center><p style="max-width:1500px; font-size:18px; margin:auto; text-align: justify; margin-bottom: 1em;"> -->
                    <!-- We show a diversity of generated results below. The rendered UV map (left) is used to define the structure of the generated video clips, while a text prompt defines the style and appearance of the clips. </p></center> -->
                <br>
                <p style="max-width:1200px; font-size:18px; margin:auto; text-align: justify; margin-bottom: 1em;">
                    Our model performs complex 3D editing using either explicit per-step instructions (<b>Top</b>) 
                    or high-level general instructions (<b>Bottom</b>). 
                    For general inputs, the system automatically decomposes the main goal into the sequence of sub-tasks 
                    displayed below the corresponding images.
                </p>

                <h3>Per-Step Instruction Results</h3>
                <div id="rumba">
                    <img src="assets/imgs/supp_final_per_step.png" width=1200px>
                </div>
                <!-- <center><p style="max-width:1500px; font-size:18px; margin:auto; text-align: justify; margin-bottom: 1em;"> -->
                    <!-- We show a diversity of generated results below. The rendered UV map (left) is used to define the structure of the generated video clips, while a text prompt defines the style and appearance of the clips. </p></center> -->

                <h3>General Instruction Results</h3>
                <div id="rumba">
                    <img src="assets/imgs/supp_final_simple.png" width=1200px>
                </div>

                <p class="section">&nbsp;</p>
                <h2>Bibtex</h2>
                <table style="width: 100%;margin-left:auto;margin-right:auto;">
                    <tbody>
                        <pre style=" display: block;
                            background: #eee;
                            white-space: pre;
                            -webkit-overflow-scrolling: touch;
                            max-width: 100%;
                            min-width: 100px;
                            border-radius: 20px;
                            text-align: left;
                            overflow: hidden;
                            ">

                        @misc{kuang2025vulcantoolaugmentedmultiagents,
                              title={VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement}, 
                              author={Zhengfei Kuang and Rui Lin and Long Zhao and Gordon Wetzstein and Saining Xie and Sanghyun Woo},
                              year={2025},
                              eprint={2512.22351},
                              archivePrefix={arXiv},
                              primaryClass={cs.CV},
                              url={https://arxiv.org/abs/2512.22351}, 
                        }

                        </pre>
                    </tbody>
                </table>
            </div>
        </div>
    <script type="text/javascript" src="script.js"></script>
    </div>
</body>
